import tiktoken
from openai import OpenAI
import json
from .config import get_config
from .text_splitter import RecursiveCharacterTextSplitter
from loguru import logger
import os
import sys
from typing import Dict, Any, Optional
import openai


# Initialize clients based on configuration
def _get_openai_client():
    """Get OpenAI client if configured"""
    config = get_config()
    if config["openai"]["api_key"]:
        return OpenAI(
            api_key=config["openai"]["api_key"],
            base_url=config["openai"]["base_url"]
        )
    return None


# Get clients
openai_client = _get_openai_client()

# Constants
MIN_CHUNK_SIZE = 140
encoder = tiktoken.get_encoding("cl100k_base")  # Using cl100k as a replacement for o200k_base

logger.info(f"OpenAI client: {openai_client}, model: {get_config()['openai']['model']}")


def get_model() -> Dict[str, Any]:
    """
    Get model configuration including client and model name.
    
    Returns:
        Dict containing model configuration
    """
    config = get_config()
    
    # Get OpenAI configuration from config
    openai_config = config.get("openai", {})
    api_key = openai_config.get("api_key", "")
    model = openai_config.get("model", "gpt-4o")
    base_url = openai_config.get("base_url", None)
    
    # Initialize OpenAI client
    client_args = {"api_key": api_key}
    if base_url:
        client_args["base_url"] = base_url
    
    client = openai.OpenAI(**client_args)
    async_client = openai.AsyncOpenAI(**client_args)
    
    return {
        "client": client,
        "async_client": async_client,
        "model": model
    }


def trim_prompt(prompt, context_size=None):
    """Trim prompt to maximum context size"""
    if context_size is None:
        context_size = get_config()["research"]["context_size"]

    if not prompt:
        return ""

    length = len(encoder.encode(prompt))
    if length <= context_size:
        return prompt

    overflow_tokens = length - context_size
    # Estimate characters to remove (3 chars per token on average)
    chunk_size = len(prompt) - overflow_tokens * 3

    if chunk_size < MIN_CHUNK_SIZE:
        return prompt[:MIN_CHUNK_SIZE]

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)
    trimmed_prompt = splitter.split_text(prompt)[0] or ""

    # Handle case where trimmed prompt is same length as original
    if len(trimmed_prompt) == len(prompt):
        return trim_prompt(prompt[:chunk_size], context_size)

    # Recursively trim until within context size
    return trim_prompt(trimmed_prompt, context_size)


def get_search_provider(search_source=None):
    """
    Get the appropriate search provider based on configuration.
    
    Returns:
        An instance of the search provider class
    """
    if search_source is None:
        config = get_config()
        search_source = config.get("research", {}).get("search_source", "serper")
    
    if search_source == "mp_search":
        from .mp_search_client import MPSearchClient
        return MPSearchClient()
    elif search_source == "tavily":
        from .tavily_client import TavilyClient
        return TavilyClient()
    else:  # Default to serper
        from .serper_client import SerperClient
        return SerperClient()
