"""
Deep research functionality for comprehensive multi-step research.
"""

import asyncio
import json
import traceback
import inspect
from typing import Optional, Callable, Dict, List, Any, Union, Tuple, AsyncGenerator
from loguru import logger

from .config import get_config
from .providers import get_search_provider
from .utils import format_source_metadata, add_event_loop_policy
from .prompts import (
    SYSTEM_PROMPT,
    SHOULD_CLARIFY_QUERY_PROMPT,
    FOLLOW_UP_QUESTIONS_PROMPT,
    PROCESS_CLARIFICATIONS_PROMPT,
    RESEARCH_PLAN_PROMPT,
    SUMMARIZE_SEARCH_RESULTS_PROMPT,
    RESEARCH_FROM_CONTENT_PROMPT,
    RESEARCH_SUMMARY_PROMPT,
    FINAL_REPORT_PROMPT,
    FINAL_ANSWER_PROMPT
)
from .model_utils import generate_completion, generate_json_completion
from .search_utils import search_and_summarize, concurrent_search


async def should_clarify_query(query: str, history_context: str = '') -> bool:
    """
    Use the language model to determine if a query needs clarification.
    """
    try:
        prompt = SHOULD_CLARIFY_QUERY_PROMPT.format(query=query, history_context=history_context)
        result = await generate_completion(prompt, temperature=0)
        logger.debug(f"query: {query}, should clarify query result: {result}")

        # 检查结果是否包含肯定回答（yes/y）
        needs_clarification = "yes" in result.lower() or 'y' in result.lower()
        return needs_clarification
    except Exception as e:
        logger.error(f"error: {str(e)}")
        return True


async def generate_followup_questions(query: str, history_context: str = '') -> Dict[str, Any]:
    """
    Generate clarifying follow-up questions for the given query.
    
    Args:
        query: The user's research query
        
    Returns:
        Dict containing whether clarification is needed and questions
    """
    try:
        # Format the prompt
        prompt = FOLLOW_UP_QUESTIONS_PROMPT.format(query=query, history_context=history_context)

        # Generate the followup questions
        result = await generate_json_completion(prompt, system_message=SYSTEM_PROMPT, temperature=0.7)

        # Ensure the expected structure
        if "needs_clarification" not in result:
            result["needs_clarification"] = False

        if "questions" not in result:
            result["questions"] = []

        logger.debug(f"Follow-up questions: {result}")
        return result

    except Exception as e:
        logger.error(f"Error generating followup questions: {str(e)}")
        return {
            "needs_clarification": False,
            "questions": []
        }


async def process_clarifications(
        query: str,
        user_responses: Dict[str, str],
        all_questions: List[Dict[str, Any]],
        history_context: str = ''
) -> Dict[str, Any]:
    """
    Process user responses to clarification questions and refine the query.
    
    Args:
        query: The original query
        user_responses: Dict mapping question keys to user responses
        all_questions: List of all questions that were asked
        
    Returns:
        Dict with refined query and other information
    """
    try:
        # Format the questions and responses for the prompt
        clarifications = []
        unanswered = []

        for question in all_questions:
            key = question.get("key", "")
            question_text = question.get("question", "")
            default = question.get("default", "")

            if key in user_responses and user_responses[key]:
                clarifications.append(f"Q: {question_text}\nA: {user_responses[key]}")
            else:
                clarifications.append(f"Q: {question_text}\nA: [Not answered by user]")
                unanswered.append(f"Q: {question_text}\nDefault: {default}")

        # Check if all questions were unanswered - if so, modify the prompt
        all_unanswered = len(unanswered) == len(all_questions) and len(all_questions) > 0

        # Format the prompt
        if all_unanswered:
            prompt = f"""
I'm reviewing a user query where they chose not to provide any clarifications.

Chat history: ```{history_context}```

Original query: {query}

The user was asked the following clarification questions but chose not to answer any:
{"\n\n".join(unanswered)}

Since the user didn't provide any clarifications, please:
1. Analyze the original query as comprehensively as possible
2. Make reasonable assumptions for all ambiguous aspects
3. Determine if this is a simple factual query that doesn't require search
4. If possible, provide a direct answer along with the refined query
- User's question is written in Chinese, 需要用中文输出.

Format your response as a valid JSON object with the following structure:
{{
  "refined_query": "The refined query with all possible considerations",
  "assumptions": ["List of all assumptions made"],
  "requires_search": true/false (boolean indicating if this query needs web search or can be answered directly),
  "direct_answer": "If requires_search is false, provide a comprehensive direct answer here, otherwise empty string"
}}

Since the user chose not to provide clarifications, be as thorough and comprehensive as possible in your analysis and answer.
"""
        else:
            prompt = PROCESS_CLARIFICATIONS_PROMPT.format(
                query=query,
                clarifications="\n\n".join(clarifications),
                unanswered_questions="\n\n".join(unanswered) if unanswered else "None",
                history_context=history_context
            )

        # Generate the clarifications processing
        result = await generate_json_completion(prompt, temperature=0.7)

        # Ensure the expected structure
        if "refined_query" not in result:
            result["refined_query"] = query

        if "assumptions" not in result:
            result["assumptions"] = []

        if "requires_search" not in result:
            result["requires_search"] = True

        if "direct_answer" not in result:
            result["direct_answer"] = ""

        logger.debug(f"Processed clarifications: {result}")
        return result

    except Exception as e:
        logger.error(f"Error processing clarifications: {str(e)}")
        return {
            "refined_query": query,
            "assumptions": [],
            "requires_search": True,
            "direct_answer": ""
        }


async def generate_research_plan(query: str, history_context: str = "") -> Dict[str, Any]:
    """
    Generate a research plan with a variable number of steps based on the query complexity.
    
    Args:
        query: The research query
        
    Returns:
        Dict containing the research plan with steps
    """
    try:
        # Format the prompt
        prompt = RESEARCH_PLAN_PROMPT.format(query=query, history_context=history_context)

        # Generate the research plan
        result = await generate_json_completion(prompt, system_message=SYSTEM_PROMPT, temperature=0.7)

        # Ensure the expected structure
        if "steps" not in result or not result["steps"]:
            # Create a default single-step plan if none was provided
            result["steps"] = [{
                "step_id": 1,
                "description": "Research the query",
                "search_query": query,
                "goal": "Find information about the query"
            }]

        if "assessments" not in result:
            result["assessments"] = "No complexity assessment provided"

        logger.debug(f"Research plan: {result}")
        return result

    except Exception as e:
        logger.error(f"Error generating research plan: {str(e)}")
        # Return a simple default plan
        return {
            "assessments": "Error occurred, using default plan",
            "steps": [{
                "step_id": 1,
                "description": "Research the query",
                "search_query": query,
                "goal": "Find information about the query"
            }]
        }


async def summarize_search_results(query: str, search_results: str) -> str:
    """
    Summarize search results for a query.
    
    Args:
        query: The search query
        search_results: Formatted search results text
        
    Returns:
        Summarized search results
    """
    try:
        # Format the prompt
        prompt = SUMMARIZE_SEARCH_RESULTS_PROMPT.format(
            query=query,
            search_results=search_results
        )

        # Generate the summary
        summary = await generate_completion(prompt, temperature=0.7)
        return summary

    except Exception as e:
        logger.error(f"Error summarizing search results: {str(e)}")
        return f"Error summarizing results for '{query}': {str(e)}"


async def research_step(
        query: str,
        depth: int,
        breadth: int,
        config: Dict[str, Any],
        on_progress: Optional[Callable] = None,
        search_provider=None,
        step_info: Dict[str, Any] = None,
        all_learnings: List[str] = None
) -> Dict[str, Any]:
    """
    Perform a single step of the research process.
    
    Args:
        query: The query to research
        depth: Current depth level
        breadth: Number of parallel queries to run
        config: Configuration dictionary
        on_progress: Optional callback for progress updates
        search_provider: Search provider instance
        step_info: Information about the current research plan step
        all_learnings: List of learnings accumulated so far
        
    Returns:
        Dict with research results
    """
    if all_learnings is None:
        all_learnings = []

    if search_provider is None:
        search_provider = get_search_provider()

    # Progress update
    if on_progress:
        progress_data = {
            "currentDepth": depth,
            "totalDepth": config["research"]["default_depth"],
            "completedQueries": 0,
            "totalQueries": breadth,
            "currentQuery": query
        }

        # Check if on_progress is a coroutine function
        if inspect.iscoroutinefunction(on_progress):
            await on_progress(progress_data)
        else:
            on_progress(progress_data)

    # Get the search results
    search_result = await search_and_summarize(query, config, search_provider)
    search_results_text = search_result["summary"]

    # Summarize the search results
    summary = await summarize_search_results(query, search_results_text)

    # If we've reached the maximum depth, just return the summary
    if depth <= 0:
        return {
            "summary": summary,
            "urls": search_result["urls"],
            "nextQueries": [],
            "learnings": []
        }

    # Figure out what to search next based on learnings so far
    prompt = RESEARCH_FROM_CONTENT_PROMPT.format(
        query=query,
        current_step=json.dumps(step_info, ensure_ascii=False) if step_info else "No specific step information",
        content=summary,
        next_queries_count=breadth
    )

    # Generate the next queries and learnings
    research_output = await generate_json_completion(prompt, temperature=0.7)

    # Extract the next queries and learnings
    next_queries = research_output.get("nextQueries", [])
    new_learnings = research_output.get("learnings", [])

    # Update learnings
    all_learnings.extend(new_learnings)

    return {
        "summary": summary,
        "urls": search_result["urls"],
        "nextQueries": next_queries,
        "learnings": new_learnings
    }


async def deep_research(
        query: str,
        breadth: int = None,
        depth: int = None,
        on_progress: Optional[Callable] = None,
        user_clarifications: Dict[str, str] = None,
        search_source: Optional[str] = None,
        history_context: Optional[str] = None
) -> Dict[str, Any]:
    """
    Perform deep research by iteratively searching based on learned information.
    
    Args:
        query: The research query
        breadth: Number of parallel queries to run at each step
        depth: Number of iterations to perform
        on_progress: Optional callback function for progress updates
        user_clarifications: User responses to clarification questions
        search_source: Optional search provider to use
        
    Returns:
        Dict with research results including learnings and visited URLs
    """
    # Load configuration
    config = get_config()

    # Use defaults from config if not specified
    if breadth is None:
        breadth = config["research"]["default_breadth"]
    if depth is None:
        depth = config["research"]["default_depth"]

    # Initialize tracking variables
    visited_urls = []
    all_learnings = []

    # Initialize search provider
    search_provider = get_search_provider(search_source=search_source)

    # Helper function to handle progress callbacks
    async def update_progress(progress_data):
        if on_progress:
            if inspect.iscoroutinefunction(on_progress):
                await on_progress(progress_data)
            else:
                on_progress(progress_data)

    try:
        # Step 1: 先判断是否需要生成澄清问题
        await update_progress({"step": "query_analysis", "message": "Analyzing query for clarity"})
        needs_clarification = False
        questions = []

        # 如果没有提供用户澄清，则判断是否需要澄清
        if not user_clarifications:
            # 修复重复调用问题 - 使用缓存变量存储结果
            _clarification_cache_key = f"should_clarify_{query}"
            if _clarification_cache_key not in globals():
                needs_clarification = await should_clarify_query(query)
                globals()[_clarification_cache_key] = needs_clarification
            else:
                needs_clarification = globals()[_clarification_cache_key]
                logger.debug(f"使用缓存的澄清结果: {query}, result: {needs_clarification}")

        # 如果LLM判断需要澄清，或者已经有用户澄清，则继续生成或处理澄清问题
        if needs_clarification or user_clarifications:
            await update_progress({"step": "clarification", "message": "Generating clarification questions"})

            followup_result = await generate_followup_questions(query, history_context)
            questions = followup_result.get("questions", [])

            # 如果有问题且没有用户回答，等待用户输入
            if needs_clarification and questions and not user_clarifications:
                await update_progress({
                    "step": "awaiting_clarification",
                    "message": "Waiting for user to answer clarification questions",
                    "questions": questions
                })
                # In a real interactive system, we would return here and wait for the user
                return {
                    "awaiting_clarification": True,
                    "questions": questions
                }

        # Step 2: Process user clarifications if provided
        refined_query = query
        user_responses = user_clarifications or {}

        if questions:
            # Track which questions were answered vs. which use defaults
            answered_questions = []
            unanswered_questions = []

            for q in questions:
                key = q.get("key", "")
                question_text = q.get("question", "")
                if key in user_responses and user_responses[key]:
                    answered_questions.append(question_text)
                else:
                    unanswered_questions.append(question_text)

            await update_progress({
                "step": "clarification_processed",
                "message": "Processing user clarifications",
                "answered": answered_questions,
                "unanswered": unanswered_questions
            })

            # Process the clarifications
            clarification_result = await process_clarifications(query, user_responses, questions, history_context)
            refined_query = clarification_result.get("refined_query", query)

            # Check if this is a simple query that can be answered directly
            if not clarification_result.get("requires_search", True):
                direct_answer = clarification_result.get("direct_answer", "")
                if direct_answer:
                    await update_progress(
                        {"step": "direct_answer", "message": "Providing direct answer without search"})
                    return {
                        "requires_search": False,
                        "direct_answer": direct_answer,
                        "learnings": [],
                        "visitedUrls": []
                    }
        else:
            # 不需要澄清，直接使用原始查询
            refined_query = query
            await update_progress(
                {"step": "clarification_skipped", "message": "Query is clear, skipping clarification"})

        # Step 3: Generate research plan with variable steps
        await update_progress({"step": "planning", "message": "Generating research plan"})

        plan_result = await generate_research_plan(refined_query, history_context)
        steps = plan_result.get("steps", [])

        # Track step summaries for the final analysis
        step_summaries = []

        # Iterate through each step in the research plan
        for step_idx, step in enumerate(steps):
            step_id = step.get("step_id", step_idx + 1)
            description = step.get("description", f"Research step {step_id}")
            search_query = step.get("search_query", refined_query)

            await update_progress({
                "currentDepth": depth,
                "totalDepth": depth,
                "completedQueries": step_idx,
                "totalQueries": len(steps),
                "currentQuery": f"Step {step_id}: {description}"
            })

            # Create a queue of queries to process for this step
            queries_to_process = [search_query]
            step_urls = []
            step_learnings = []

            # Process the initial query and follow-ups
            current_depth = depth
            total_processed = 0

            while queries_to_process and current_depth > 0:
                # Get the next queries to process (up to breadth)
                current_queries = queries_to_process[:breadth]
                queries_to_process = queries_to_process[breadth:]

                # Research these queries concurrently
                await update_progress({
                    "currentDepth": current_depth,
                    "totalDepth": depth,
                    "completedQueries": total_processed,
                    "totalQueries": len(queries_to_process) + len(current_queries),
                    "currentQuery": f"Processing {len(current_queries)} queries concurrently"
                })

                # Process each query in the current batch
                research_tasks = []
                for current_query in current_queries:
                    task = research_step(
                        query=current_query,
                        depth=current_depth,
                        breadth=breadth,
                        config=config,
                        on_progress=on_progress,
                        search_provider=search_provider,
                        step_info=step,
                        all_learnings=all_learnings
                    )
                    research_tasks.append(task)

                # Execute tasks with concurrency
                results = await asyncio.gather(*research_tasks)

                # Process the results
                for result in results:
                    # Update tracking variables
                    step_urls.extend(result["urls"])
                    step_learnings.extend(result["learnings"])
                    total_processed += 1

                    # Add the new queries to our queue
                    next_queries = result["nextQueries"]
                    queries_to_process.extend(next_queries)

                # Decrement depth for next iteration
                current_depth -= 1

            # Update visited URLs and learnings
            visited_urls.extend(step_urls)
            all_learnings.extend(step_learnings)

            # Save step summary
            step_summaries.append({
                "step_id": step_id,
                "description": description,
                "learnings": step_learnings,
                "urls": step_urls
            })

        # Perform final analysis
        steps_summary_text = "\n\n".join([
            f"Step {s['step_id']}: {s['description']}\nLearnings: {json.dumps(s['learnings'])}"
            for s in step_summaries
        ])

        final_prompt = RESEARCH_SUMMARY_PROMPT.format(
            query=refined_query,
            steps_summary=steps_summary_text
        )

        final_result = await generate_json_completion(final_prompt, temperature=0.7)
        logger.debug(f"Final research result: {final_result}")

        # Add final findings to learnings
        if "findings" in final_result:
            all_learnings.extend(final_result["findings"])

        # Return compiled results
        return {
            "query": refined_query,
            "originalQuery": query,
            "learnings": list(set(all_learnings)),  # Remove duplicates
            "visitedUrls": list(set(visited_urls)),  # Remove duplicates
            "summary": final_result
        }

    except Exception as e:
        logger.error(f"Error in deep research: {str(e)}")
        logger.error(traceback.format_exc())

        return {
            "error": str(e),
            "learnings": all_learnings,
            "visitedUrls": visited_urls
        }


def deep_research_sync(
        query: str,
        breadth: int = None,
        depth: int = None,
        on_progress: Optional[Callable] = None,
        user_clarifications: Dict[str, str] = None,
        search_source: Optional[str] = None,
        history_context: Optional[str] = None
) -> Dict[str, Any]:
    """
    Synchronous wrapper for the deep_research function.
    
    Args:
        query: The research query
        breadth: Number of parallel queries to run at each step
        depth: Number of iterations to perform
        on_progress: Optional callback function for progress updates
        user_clarifications: User responses to clarification questions
        search_source: Optional search provider to use
        
    Returns:
        Dict with research results
    """
    # Add event loop policy for Windows if needed
    add_event_loop_policy()

    # Run the async function in the event loop
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(
        deep_research(
            query=query,
            breadth=breadth,
            depth=depth,
            on_progress=on_progress,
            user_clarifications=user_clarifications,
            search_source=search_source,
            history_context=history_context
        )
    )


async def write_final_report(query: str, learnings: List[str], visited_urls: List[str], history_context: str = '') -> str:
    """
    Generate a final research report based on learnings and sources.
    
    Args:
        query: The original research query
        learnings: List of key learnings/facts discovered
        visited_urls: List of sources consulted
        history_context: chat history
        
    Returns:
        A formatted research report
    """
    formatted_prompt = FINAL_REPORT_PROMPT.format(
        prompt=query,
        learnings=' '.join(learnings),
        sources=' '.join(visited_urls),
        history_context=history_context
    )

    report = await generate_completion(
        prompt=formatted_prompt,
        system_message=SYSTEM_PROMPT,
        temperature=0.7
    )

    return report


async def write_final_answer(query: str, learnings: List[str], history_context: str = '') -> str:
    """
    Generate a final concise answer based on the research.
    
    Args:
        query: The original research query
        learnings: List of key learnings/facts discovered
        
    Returns:
        A concise answer to the query
    """
    formatted_prompt = FINAL_ANSWER_PROMPT.format(
        prompt=query,
        learnings=' '.join(learnings),
        history_context=history_context
    )

    answer = await generate_completion(
        prompt=formatted_prompt,
        system_message=SYSTEM_PROMPT,
        temperature=0.7
    )

    return answer


def write_final_report_sync(query: str, learnings: List[str], visited_urls: List[str], history_context: str = '') -> str:
    """
    Synchronous wrapper for the write_final_report function.
    
    Args:
        query: The original research query
        learnings: List of key learnings/facts discovered
        visited_urls: List of sources consulted
        
    Returns:
        A formatted research report
    """
    # Add event loop policy for Windows if needed
    add_event_loop_policy()

    # Run the async function in the event loop
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(
        write_final_report(
            query=query,
            learnings=learnings,
            visited_urls=visited_urls,
            history_context=history_context
        )
    )


def write_final_answer_sync(query: str, learnings: List[str], history_context: str = '') -> str:
    """
    Synchronous wrapper for the write_final_answer function.
    
    Args:
        query: The original research query
        learnings: List of key learnings/facts discovered
        
    Returns:
        A concise answer to the query
    """
    # Add event loop policy for Windows if needed
    add_event_loop_policy()

    # Run the async function in the event loop
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(
        write_final_answer(
            query=query,
            learnings=learnings,
            history_context=history_context
        )
    )


async def deep_research_stream(
        query: str,
        breadth: int = None,
        depth: int = None,
        on_progress: Optional[Callable] = None,
        user_clarifications: Dict[str, str] = None,
        search_source: Optional[str] = None,
        history_context: Optional[str] = None,
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Streaming version of deep research that yields partial results.
    
    Args:
        query: The research query
        breadth: Number of parallel queries to run at each step
        depth: Number of iterations to perform
        on_progress: Optional callback function for progress updates
        user_clarifications: User responses to clarification questions
        search_source: Optional search provider to use
        history_context: history chat content
        
    Yields:
        Dict with partial research results and status updates
    """
    # Load configuration
    config = get_config()

    # Use defaults from config if not specified
    if breadth is None:
        breadth = config["research"]["default_breadth"]
    if depth is None:
        depth = config["research"]["default_depth"]

    # Initialize tracking variables
    visited_urls = []
    all_learnings = []

    # Initialize search provider
    search_provider = get_search_provider(search_source=search_source)

    try:
        # Step 1: Yield initial status
        yield {
            "status_update": f"开始研究查询: '{query}'...",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "current_query": query,
            "stage": "initial"
        }

        # Step 1.5: 先判断是否需要生成澄清问题
        needs_clarification = False

        # 如果没有提供用户澄清，则判断是否需要澄清
        if not user_clarifications:
            yield {
                "status_update": f"分析查询是否需要澄清...",
                "learnings": all_learnings,
                "visitedUrls": visited_urls,
                "current_query": query,
                "stage": "analyzing_query"
            }
            
            # 修复重复调用问题 - 使用缓存变量存储结果
            _clarification_cache_key = f"should_clarify_{query}"
            if _clarification_cache_key not in globals():
                needs_clarification = await should_clarify_query(query, history_context)
                globals()[_clarification_cache_key] = needs_clarification
            else:
                needs_clarification = globals()[_clarification_cache_key]
                logger.debug(f"使用缓存的澄清结果: {query}, result: {needs_clarification}")

            if not needs_clarification:
                yield {
                    "status_update": f"查询已足够清晰，跳过澄清步骤",
                    "learnings": all_learnings,
                    "visitedUrls": visited_urls,
                    "current_query": query,
                    "stage": "clarification_skipped"
                }

        # 如果LLM判断需要澄清，或者已经有用户澄清，则继续生成或处理澄清问题
        questions = []
        if needs_clarification or user_clarifications:
            # Step 2: Generate clarification questions if needed
            if needs_clarification:
                yield {
                    "status_update": f"生成澄清问题...",
                    "learnings": all_learnings,
                    "visitedUrls": visited_urls,
                    "current_query": query,
                    "stage": "generating_questions"
                }

                followup_result = await generate_followup_questions(query, history_context)
                questions = followup_result.get("questions", [])

                if questions:
                    # If clarification is needed, update status
                    yield {
                        "status_update": f"查询需要澄清，生成了 {len(questions)} 个问题",
                        "learnings": all_learnings,
                        "visitedUrls": visited_urls,
                        "current_query": query,
                        "questions": questions,
                        "stage": "clarification_needed"
                    }

                    # If we don't have user responses yet, wait for them
                    if not user_clarifications:
                        yield {
                            "status_update": "等待用户回答澄清问题...",
                            "learnings": all_learnings,
                            "visitedUrls": visited_urls,
                            "current_query": query,
                            "questions": questions,
                            "awaiting_clarification": True,
                            "stage": "awaiting_clarification"
                        }
                        return

        # Step 3: Process user clarifications if provided
        refined_query = query
        user_responses = user_clarifications or {}

        if questions and user_clarifications:
            # Track which questions were answered vs. which use defaults
            answered_questions = []
            unanswered_questions = []

            for q in questions:
                key = q.get("key", "")
                question_text = q.get("question", "")
                if key in user_responses and user_responses[key]:
                    answered_questions.append(question_text)
                else:
                    unanswered_questions.append(question_text)

            yield {
                "status_update": f"处理用户的澄清回答 ({len(answered_questions)}/{len(questions)} 已回答)",
                "learnings": all_learnings,
                "visitedUrls": visited_urls,
                "current_query": query,
                "answered_questions": answered_questions,
                "unanswered_questions": unanswered_questions,
                "stage": "processing_clarifications"
            }

            # Process the clarifications
            clarification_result = await process_clarifications(query, user_responses, questions, history_context)
            refined_query = clarification_result.get("refined_query", query)

            yield {
                "status_update": f"查询已优化: '{refined_query}'",
                "learnings": all_learnings,
                "visitedUrls": visited_urls,
                "original_query": query,
                "current_query": refined_query,
                "assumptions": clarification_result.get("assumptions", []),
                "stage": "query_refined"
            }

            # Check if this is a simple query that can be answered directly
            if not clarification_result.get("requires_search", True):
                direct_answer = clarification_result.get("direct_answer", "")
                if direct_answer:
                    yield {
                        "status_update": "查询可以直接回答，无需搜索",
                        "requires_search": False,
                        "direct_answer": direct_answer,
                        "final_report": direct_answer,  # 直接使用direct_answer作为最终报告
                        "learnings": ["直接回答: " + direct_answer],
                        "visitedUrls": [],
                        "stage": "completed"
                    }
                    return  # 不再需要继续执行查询流程

        # Step 4: Generate research plan with variable steps
        yield {
            "status_update": f"为查询生成研究计划: '{refined_query}'",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "current_query": refined_query,
            "stage": "planning"
        }

        plan_result = await generate_research_plan(refined_query, history_context)
        steps = plan_result.get("steps", [])

        yield {
            "status_update": f"生成了 {len(steps)} 步研究计划: {plan_result.get('assessments', '无评估')}",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "current_query": refined_query,
            "research_plan": steps,
            "stage": "plan_generated"
        }

        # Track step summaries for the final analysis
        step_summaries = []

        # Iterate through each step in the research plan
        for step_idx, step in enumerate(steps):
            step_id = step.get("step_id", step_idx + 1)
            description = step.get("description", f"Research step {step_id}")
            search_query = step.get("search_query", refined_query)

            yield {
                "status_update": f"开始研究步骤 {step_id}/{len(steps)}: {description}",
                "learnings": all_learnings,
                "visitedUrls": visited_urls,
                "current_query": refined_query,
                "step_query": search_query,
                "current_step": step,
                "progress": {
                    "current_step": step_id,
                    "total_steps": len(steps)
                },
                "stage": "step_started"
            }

            # Create a queue of queries to process for this step
            queries_to_process = [search_query]
            step_urls = []
            step_learnings = []

            # Process the initial query and follow-ups
            current_depth = depth
            total_processed = 0

            while queries_to_process and current_depth > 0:
                # Get the next queries to process (up to breadth)
                current_queries = queries_to_process[:breadth]
                remaining_queries = len(queries_to_process) - len(current_queries)
                queries_to_process = queries_to_process[breadth:]

                # Research these queries concurrently
                yield {
                    "status_update": f"步骤 {step_id}/{len(steps)}: 并行研究 {len(current_queries)} 个查询，剩余队列: {remaining_queries}",
                    "learnings": all_learnings,
                    "visitedUrls": visited_urls,
                    "current_queries": current_queries,
                    "depth_level": current_depth,
                    "max_depth": depth,
                    "progress": {
                        "current_step": step_id,
                        "total_steps": len(steps),
                        "current_depth": current_depth,
                        "max_depth": depth,
                        "processed_queries": total_processed,
                        "remaining_queries": remaining_queries
                    },
                    "stage": "processing_queries"
                }

                # Process each query in the current batch
                research_tasks = []
                for current_query in current_queries:
                    task = research_step(
                        query=current_query,
                        depth=current_depth,
                        breadth=breadth,
                        config=config,
                        on_progress=on_progress,
                        search_provider=search_provider,
                        step_info=step,
                        all_learnings=all_learnings
                    )
                    research_tasks.append(task)

                # Execute tasks with concurrency
                results = await asyncio.gather(*research_tasks)

                # Process the results
                new_learnings = []
                new_urls = []
                for result in results:
                    # Update tracking variables
                    step_urls.extend(result["urls"])
                    new_urls.extend(result["urls"])
                    step_learnings.extend(result["learnings"])
                    new_learnings.extend(result["learnings"])
                    total_processed += 1

                    # Add the new queries to our queue
                    next_queries = result["nextQueries"]
                    queries_to_process.extend(next_queries)

                # Decrement depth for next iteration
                current_depth -= 1

                # Format learnings and URLs for display
                formatted_learnings = []
                for i, learning in enumerate(new_learnings):
                    formatted_learnings.append(f"[{i + 1}] {learning}")

                formatted_urls = []
                for i, url in enumerate(new_urls):
                    formatted_urls.append(f"[{i + 1}] {url}")

                # Yield intermediate results
                yield {
                    "status_update": f"步骤 {step_id}/{len(steps)}: 发现 {len(new_learnings)} 个新见解",
                    "learnings": all_learnings + step_learnings,
                    "visitedUrls": visited_urls + step_urls,
                    "new_learnings": new_learnings,
                    "formatted_new_learnings": formatted_learnings,
                    "new_urls": new_urls,
                    "formatted_new_urls": formatted_urls,
                    "next_queries": queries_to_process[:3],  # Show a few upcoming queries
                    "progress": {
                        "current_step": step_id,
                        "total_steps": len(steps),
                        "current_depth": current_depth,
                        "max_depth": depth,
                        "processed_queries": total_processed
                    },
                    "stage": "insights_found"
                }

            # Update visited URLs and learnings
            visited_urls.extend(step_urls)
            all_learnings.extend(step_learnings)

            # Save step summary
            step_summaries.append({
                "step_id": step_id,
                "description": description,
                "learnings": step_learnings,
                "urls": step_urls
            })

            # Format all step learnings for display
            formatted_step_learnings = []
            for i, learning in enumerate(step_learnings):
                formatted_step_learnings.append(f"[{i + 1}] {learning}")

            # Step completion update
            yield {
                "status_update": f"完成研究步骤 {step_id}/{len(steps)}: {description}，获得 {len(step_learnings)} 个见解",
                "learnings": all_learnings,
                "visitedUrls": visited_urls,
                "step_learnings": step_learnings,
                "formatted_step_learnings": formatted_step_learnings,
                "step_urls": step_urls,
                "progress": {
                    "current_step": step_id,
                    "total_steps": len(steps),
                    "completed": True
                },
                "stage": "step_completed"
            }

        # Perform final analysis
        yield {
            "status_update": "分析所有已收集的信息...",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "stage": "final_analysis"
        }

        steps_summary_text = "\n\n".join([
            f"步骤 {s['step_id']}: {s['description']}\n发现: {json.dumps(s['learnings'], ensure_ascii=False)}"
            for s in step_summaries
        ])

        final_prompt = RESEARCH_SUMMARY_PROMPT.format(
            query=refined_query,
            steps_summary=steps_summary_text
        )

        final_result = await generate_json_completion(final_prompt, temperature=0.7)

        # Add final findings to learnings
        final_findings = []
        if "findings" in final_result:
            all_learnings.extend(final_result["findings"])
            final_findings = final_result["findings"]

        # Format final findings
        formatted_findings = []
        for i, finding in enumerate(final_findings):
            formatted_findings.append(f"[{i + 1}] {finding}")

        yield {
            "status_update": f"分析完成，得出 {len(final_findings)} 个主要发现",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "final_findings": final_findings,
            "formatted_final_findings": formatted_findings,
            "gaps": final_result.get("gaps", []),
            "recommendations": final_result.get("recommendations", []),
            "stage": "analysis_completed"
        }
        final_answer = ""
        yield {
            "status_update": "生成详细研究报告...",
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "final_answer": final_answer,
            "stage": "generating_report"
        }

        # Generate report (non-streaming for now)
        final_report = await write_final_report(refined_query, all_learnings, visited_urls, history_context)

        # Return compiled results
        yield {
            "status_update": "研究完成!",
            "query": refined_query,
            "originalQuery": query,
            "learnings": list(set(all_learnings)),  # Remove duplicates
            "visitedUrls": list(set(visited_urls)),  # Remove duplicates
            "summary": final_result,
            "final_answer": final_answer,
            "final_report": final_report,
            "stage": "completed"
        }

    except Exception as e:
        logger.error(f"Error in deep research stream: {str(e)}")
        logger.error(traceback.format_exc())

        yield {
            "status_update": f"错误: {str(e)}",
            "error": str(e),
            "learnings": all_learnings,
            "visitedUrls": visited_urls,
            "stage": "error"
        }


async def write_final_report_stream(prompt: str, learnings: List[str], visited_urls: List[str], history_context: str = '') -> AsyncGenerator[
    str, None]:
    """
    Streaming version of write_final_report that yields chunks of the report.
    
    Args:
        prompt: The original research query
        learnings: List of key learnings/facts discovered
        visited_urls: List of sources consulted
        
    Yields:
        Chunks of the final report
    """
    formatted_prompt = FINAL_REPORT_PROMPT.format(
        prompt=prompt,
        learnings=' '.join(learnings),
        sources=' '.join(visited_urls),
        history_context=history_context
    )

    response_generator = await generate_completion(
        prompt=formatted_prompt,
        system_message="You are an expert researcher providing detailed, well-structured reports in Chinese.",
        temperature=0.7,
        stream=True
    )

    # Stream the response chunks
    async for chunk in response_generator:
        yield chunk


async def write_final_answer_stream(prompt: str, learnings: List[str], history_context: str = '') -> AsyncGenerator[str, None]:
    """
    Streaming version of write_final_answer that yields chunks of the answer.
    
    Args:
        prompt: The original research query
        learnings: List of key learnings/facts discovered
        
    Yields:
        Chunks of the final answer
    """
    formatted_prompt = FINAL_ANSWER_PROMPT.format(
        prompt=prompt,
        learnings=' '.join(learnings),
        history_context=history_context
    )

    response_generator = await generate_completion(
        prompt=formatted_prompt,
        system_message="You are an expert researcher providing concise answers based on research findings in Chinese.",
        temperature=0.7,
        stream=True
    )

    # Stream the response chunks
    async for chunk in response_generator:
        yield chunk
