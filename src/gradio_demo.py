import gradio as gr
import asyncio
import time
from loguru import logger

from .config import get_config
from .deep_research import deep_research_sync, write_final_report, write_final_answer
from .providers import get_model


def run_gradio_demo():
    """Run the Gradio demo interface"""

    def research_and_generate(
            query: str,
            breadth: int,
            depth: int,
            output_mode: str,
            progress=gr.Progress()
    ):
        """
        Run research and generate report/answer
        
        Args:
            query: Research query
            breadth: Research breadth
            depth: Research depth
            output_mode: "report" or "answer"
            progress: Gradio progress indicator
        """
        progress(0, desc="Starting research...")

        # Progress callback for updating the UI
        def progress_callback(progress_data):
            current_depth = progress_data["currentDepth"]
            total_depth = progress_data["totalDepth"]
            completed = progress_data["completedQueries"]
            total = progress_data["totalQueries"] or 1  # Avoid division by zero
            current = progress_data["currentQuery"]

            # Calculate overall progress (0-1)
            depth_progress = (total_depth - current_depth) / total_depth
            query_progress = completed / total
            overall = (depth_progress + query_progress) / 2

            progress(overall, desc=f"Depth {current_depth}/{total_depth}, Query {completed}/{total}: {current}")

        # Run the research
        try:
            result = deep_research_sync(
                query=query,
                breadth=breadth,
                depth=depth,
                on_progress=progress_callback
            )

            learnings = result["learnings"]
            visited_urls = result["visitedUrls"]

            progress(0.9, desc="Generating final output...")

            # Generate the final output based on mode
            if output_mode == "report":
                # Use the synchronous wrapper for the final report
                final_output = asyncio.run(write_final_report(
                    prompt=query,
                    learnings=learnings,
                    visited_urls=visited_urls
                ))
            else:
                # Use the synchronous wrapper for the final answer
                final_output = asyncio.run(write_final_answer(
                    prompt=query,
                    learnings=learnings
                ))

            progress(1.0, desc="Complete!")

            return final_output, "\n\n".join([f"- {learning}" for learning in learnings]), "\n".join(
                [f"- {url}" for url in visited_urls])

        except Exception as e:
            logger.error(f"Error in research: {str(e)}")
            return f"Error: {str(e)}", "", ""

    # Create the Gradio interface
    with gr.Blocks(title="Deep Research", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ” Deep Research")
        gr.Markdown("AI-powered research assistant that performs iterative, deep research on any topic.")

        with gr.Row():
            with gr.Column(scale=2):
                query_input = gr.Textbox(
                    label="ç ”ç©¶é—®é¢˜",
                    placeholder="è¾“å…¥æ‚¨æƒ³è¦ç ”ç©¶çš„é—®é¢˜...",
                    lines=3
                )

                with gr.Row():
                    config = get_config()
                    default_breadth = config["research"]["default_breadth"]
                    default_depth = config["research"]["default_depth"]

                    breadth_input = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=default_breadth,
                        step=1,
                        label="å¹¿åº¦ (æ¯æ¬¡è¿­ä»£çš„æœç´¢æŸ¥è¯¢æ•°é‡)"
                    )

                    depth_input = gr.Slider(
                        minimum=1,
                        maximum=5,
                        value=default_depth,
                        step=1,
                        label="æ·±åº¦ (é€’å½’è¿­ä»£æ¬¡æ•°)"
                    )

                with gr.Row():
                    output_mode = gr.Radio(
                        choices=["report", "answer"],
                        value="report",
                        label="è¾“å‡ºæ¨¡å¼",
                        info="æŠ¥å‘Š (è¯¦ç»†) æˆ– å›ç­” (ç®€æ´)"
                    )

                    stream_mode = gr.Checkbox(
                        label="æµå¼è¾“å‡º",
                        value=True,
                        info="å¯ç”¨æµå¼è¾“å‡ºä»¥å®æ—¶æŸ¥çœ‹ç»“æœ"
                    )

                research_button = gr.Button("å¼€å§‹ç ”ç©¶", variant="primary")

            with gr.Column(scale=3):
                output = gr.Markdown(label="ç ”ç©¶ç»“æœ")

                with gr.Accordion("å…³é”®å‘ç°", open=False):
                    learnings_output = gr.Markdown()

                with gr.Accordion("æ¥æº", open=False):
                    sources_output = gr.Markdown()

        # Define the click event with streaming support
        @research_button.click(
            inputs=[query_input, breadth_input, depth_input, output_mode, stream_mode],
            outputs=[output, learnings_output, sources_output]
        )
        def on_research_click(query, breadth, depth, output_mode, stream):
            if not query:
                return "è¯·è¾“å…¥ç ”ç©¶é—®é¢˜", "", ""

            if not stream:
                return research_and_generate(query, breadth, depth, output_mode)

            # For streaming mode, we need to use a generator
            progress_state = {
                "currentDepth": 0,
                "totalDepth": depth,
                "completedQueries": 0,
                "totalQueries": 0,
                "currentQuery": None
            }

            learnings_text = ""
            sources_text = ""

            # Yield initial state
            yield "æ­£åœ¨å¼€å§‹ç ”ç©¶...", "", ""

            # Progress callback for updating the UI
            def progress_callback(progress_data):
                nonlocal progress_state
                progress_state.update(progress_data)

            # Run the research (non-streaming part)
            try:
                result = deep_research_sync(
                    query=query,
                    breadth=breadth,
                    depth=depth,
                    on_progress=progress_callback
                )

                learnings = result["learnings"]
                visited_urls = result["visitedUrls"]

                # Format learnings and sources
                learnings_text = "\n\n".join([f"- {learning}" for learning in learnings])
                sources_text = "\n".join([f"- {url}" for url in visited_urls])

                # Yield progress update
                yield "ç ”ç©¶å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...", learnings_text, sources_text

                # Generate the final output based on mode
                model_config = get_model()

                if output_mode == "report":
                    prompt_text = f"""I've been researching the following topic: {query}

Here are the key learnings from my research:
{' '.join(learnings)}

Here are the sources I've consulted:
{' '.join(visited_urls)}

Please write a comprehensive research report on this topic, incorporating the learnings and citing the sources where appropriate. The report should be well-structured with headings, subheadings, and a conclusion.
- User's question is written in Chinese, éœ€è¦ç”¨ä¸­æ–‡è¾“å‡º.
"""
                else:
                    prompt_text = f"""I've been researching the following topic: {query}

Here are the key learnings from my research:
{' '.join(learnings)}

Please provide a concise answer to the original query based on these learnings.
- User's question is written in Chinese, éœ€è¦ç”¨ä¸­æ–‡è¾“å‡º.
"""

                # Stream the response
                streamed_content = ""

                # Create streaming completion
                response = model_config["client"].chat.completions.create(
                    model=model_config["model"],
                    messages=[
                        {"role": "system",
                         "content": "You are an expert researcher providing detailed, well-structured reports in Chinese."},
                        {"role": "user", "content": prompt_text}
                    ],
                    temperature=0.7,
                    stream=True
                )

                # Process streaming response
                for chunk in response:
                    if hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content') and \
                            chunk.choices[0].delta.content:
                        content_chunk = chunk.choices[0].delta.content
                        streamed_content += content_chunk

                        # Yield updated content
                        yield streamed_content, learnings_text, sources_text

                        # Small delay to make streaming visible
                        time.sleep(0.01)

                # Final yield with complete content
                yield streamed_content, learnings_text, sources_text

            except Exception as e:
                logger.error(f"Error in streaming research: {str(e)}")
                yield f"é”™è¯¯: {str(e)}", "", ""

        # Add examples
        gr.Examples(
            examples=[
                ["ä¸­å›½å†å²ä¸Šæœ€ä¼Ÿå¤§çš„å‘æ˜æ˜¯ä»€ä¹ˆï¼Ÿ", 3, 2, "report", True],
                ["äººå·¥æ™ºèƒ½ä¼šåœ¨æœªæ¥åå¹´å†…å–ä»£å“ªäº›å·¥ä½œï¼Ÿ", 4, 2, "report", True],
                ["å¦‚ä½•æœ‰æ•ˆå­¦ä¹ ä¸€é—¨æ–°è¯­è¨€ï¼Ÿ", 3, 2, "answer", True],
            ],
            inputs=[query_input, breadth_input, depth_input, output_mode, stream_mode]
        )

    # Launch the demo
    demo.launch(server_name="0.0.0.0", share=False)
