# -*- coding: utf-8 -*-
"""
@author:XuMing(xuming624@qq.com)
@description:

A simplified Gradio demo for Deep Research with basic conversation interface.
This version uses the latest Gradio features with ChatMessage for a modern UI.
"""

import time
import gradio as gr
from gradio import ChatMessage
from loguru import logger
from .config import get_config
from .deep_research import (
    deep_research_stream,
    generate_followup_questions,
    process_clarifications,
    write_final_report_stream,
    should_clarify_query
)


async def handle_research_progress(partial_result, thinking_msg, log_msg, conversation_state):
    """å¤„ç†ç ”ç©¶è¿›åº¦å’ŒçŠ¶æ€æ›´æ–°çš„å…±åŒé€»è¾‘"""
    if partial_result.get("status_update"):
        status = partial_result.get("status_update")
        stage = partial_result.get("stage", "")

        # è·³è¿‡æ¾„æ¸…ç›¸å…³çš„é˜¶æ®µ
        if stage in ["analyzing_query", "clarification_needed", "awaiting_clarification"]:
            return None

        # æ£€æŸ¥çŠ¶æ€æ˜¯å¦æœ‰å˜åŒ–
        if status != conversation_state["last_status"]:
            # è®°å½•æ–°çŠ¶æ€
            conversation_state["last_status"] = status
            thinking_msg.content = status

            # ä¿å­˜ç ”ç©¶è®¡åˆ’
            if stage == "plan_generated" and partial_result.get("research_plan"):
                research_plan = partial_result.get("research_plan")
                plan_text = "### ç ”ç©¶è®¡åˆ’\n"
                for i, step in enumerate(research_plan):
                    step_id = step.get("step_id", i + 1)
                    description = step.get("description", "")
                    search_query = step.get("search_query", "")
                    goal = step.get("goal", "")
                    plan_text += f"**æ­¥éª¤ {step_id}**: {description}\n- æŸ¥è¯¢: {search_query}\n- ç›®æ ‡: {goal}\n\n"

                log_msg.content += f"\n\n{plan_text}"
                return [thinking_msg, log_msg]

            # æ›´æ–°æ—¥å¿—æ¶ˆæ¯
            timestamp = time.strftime('%H:%M:%S')
            log_msg.content += f"\n\n### [{timestamp}] {status}\n"

            # æ˜¾ç¤ºå½“å‰ç ”ç©¶è®¡åˆ’æ­¥éª¤
            if partial_result.get("current_step"):
                current_step = partial_result.get("current_step")
                step_id = current_step.get("step_id", "")
                description = current_step.get("description", "")
                log_msg.content += f"\n**å½“å‰æ­¥éª¤ {step_id}**: {description}\n"

            # æ˜¾ç¤ºå½“å‰æŸ¥è¯¢
            if partial_result.get("current_queries"):
                queries = partial_result.get("current_queries")
                log_msg.content += "\n**å½“å‰å¹¶è¡ŒæŸ¥è¯¢**:\n"
                for i, q in enumerate(queries, 1):
                    log_msg.content += f"{i}. {q}\n"
            elif partial_result.get("step_query"):
                log_msg.content += f"\n**å½“å‰æŸ¥è¯¢**: {partial_result.get('step_query')}\n"
            elif partial_result.get("current_query"):
                log_msg.content += f"\n**å½“å‰æŸ¥è¯¢**: {partial_result.get('current_query')}\n"

            # æ·»åŠ é˜¶æ®µè¯¦ç»†ä¿¡æ¯
            if stage == "insights_found" and partial_result.get("formatted_new_learnings"):
                log_msg.content += "\n**æ–°è§è§£**:\n" + "\n".join(
                    partial_result.get("formatted_new_learnings", []))
                if partial_result.get("formatted_new_urls") and len(
                        partial_result.get("formatted_new_urls")) > 0:
                    log_msg.content += "\n\n**æ¥æº**:\n" + "\n".join(
                        partial_result.get("formatted_new_urls", [])[:3])

            elif stage == "step_completed" and partial_result.get("formatted_step_learnings"):
                log_msg.content += "\n**æ­¥éª¤æ€»ç»“**:\n" + "\n".join(
                    partial_result.get("formatted_step_learnings", []))

            elif stage == "analysis_completed" and partial_result.get("formatted_final_findings"):
                log_msg.content += "\n**ä¸»è¦å‘ç°**:\n" + "\n".join(
                    partial_result.get("formatted_final_findings", []))

                if partial_result.get("gaps"):
                    log_msg.content += "\n\n**ç ”ç©¶ç©ºç™½**:\n- " + "\n- ".join(partial_result.get("gaps", []))

            # æ·»åŠ è¿›åº¦ä¿¡æ¯
            if partial_result.get("progress"):
                progress = partial_result.get("progress")
                if "current_step" in progress and "total_steps" in progress:
                    log_msg.content += f"\n\n**è¿›åº¦**: æ­¥éª¤ {progress['current_step']}/{progress['total_steps']}"
                    if "processed_queries" in progress:
                        log_msg.content += f", å·²å¤„ç† {progress['processed_queries']} ä¸ªæŸ¥è¯¢"

            return [thinking_msg, log_msg]
    return None


# Load configuration
config = get_config()


def run_gradio_demo():
    """Run a modern Gradio demo for Deep Research using ChatMessage"""

    # Conversation state (shared across functions)
    conversation_state = {
        "current_query": "",
        "needs_clarification": False,
        "questions": [],
        "waiting_for_clarification": False,
        "clarification_answers": {},
        "report_mode": True,  # æ€»æ˜¯ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        "show_details": True,  # æ€»æ˜¯æ˜¾ç¤ºç ”ç©¶è¯¦æƒ…
        "last_status": "",  # è·Ÿè¸ªæœ€åä¸€ä¸ªçŠ¶æ€æ›´æ–°
        "search_source": "tavily",  # é»˜è®¤æœç´¢æä¾›å•†
        "history_chat": []
    }

    async def research_with_thinking(message, history, search_source):
        """Process the query with progressive thinking steps shown in the UI"""
        if not message:
            yield history
            return

        logger.debug(f"Starting research, message: {message}, history: {history}, "
                     f"search_source: {search_source}")
        conversation_state["search_source"] = search_source

        # é‡ç½®æœ€åçŠ¶æ€
        conversation_state["last_status"] = ""

        # æå–å†å²å¯¹è¯ä¸­çš„ç”¨æˆ·è¾“å…¥
        history_context = ''
        for msg in history:
            if msg.get("role") == "user":
                q = 'Q:' + msg.get("content", "") + '\n'
                history_context += q

        # è®°å½•å†å²å¯¹è¯åˆ°conversation_state
        conversation_state["history_context"] = history_context

        # Check if this is a clarification answer
        if conversation_state["waiting_for_clarification"]:
            async for response in handle_clarification_answer(message, history_context):
                yield response
            return

        # Start with a thinking message - only show the current step
        thinking_msg = ChatMessage(
            content="",
            metadata={"title": "_ç ”ç©¶ä¸­_", "id": 0, "status": "pending"}
        )

        # æ€»æ˜¯æ·»åŠ å•ç‹¬çš„ç ”ç©¶æ—¥å¿—æ¶ˆæ¯
        log_msg = ChatMessage(
            content="## ç ”ç©¶è¿‡ç¨‹è¯¦æƒ…\n\n_å®æ—¶è®°å½•ç ”ç©¶æ­¥éª¤å’Œå‘ç°_",
            metadata={"title": "_ç ”ç©¶æ—¥å¿—_", "id": 100}
        )
        yield [thinking_msg, log_msg]

        # åœ¨Gradioç•Œé¢ä¸­å‘ŠçŸ¥ç”¨æˆ·æˆ‘ä»¬æ­£åœ¨åˆ†ææŸ¥è¯¢
        thinking_msg.content = "åˆ†ææŸ¥è¯¢éœ€æ±‚ä¸­..."
        log_msg.content += "\n\n### æŸ¥è¯¢å¤„ç†\n**æ“ä½œ**: åˆ†ææŸ¥è¯¢æ˜¯å¦éœ€è¦æ¾„æ¸…\n"
        yield [thinking_msg, log_msg]

        # éœ€è¦åœ¨Gradioç•Œé¢ä¸­æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
        needs_clarification = await should_clarify_query(message, history_context)
        if needs_clarification:
            # éœ€è¦æ¾„æ¸…ï¼Œç”Ÿæˆé—®é¢˜å¹¶ç­‰å¾…ç”¨æˆ·å›ç­”
            thinking_msg.content = "ç”Ÿæˆæ¾„æ¸…é—®é¢˜..."
            log_msg.content += "\n\n### æŸ¥è¯¢åˆ†æ\n**ç»“æœ**: éœ€è¦æ¾„æ¸…\n"
            yield [thinking_msg, log_msg]

            followup_result = await generate_followup_questions(message, history_context)
            questions = followup_result.get("questions", [])

            if questions:
                # ä¿å­˜é—®é¢˜å’ŒçŠ¶æ€
                conversation_state["current_query"] = message
                conversation_state["questions"] = questions
                conversation_state["waiting_for_clarification"] = True

                # æ˜¾ç¤ºé—®é¢˜ç»™ç”¨æˆ·
                thinking_msg.content = "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œå¸®åŠ©æˆ‘æ›´å¥½åœ°ç†è§£æ‚¨çš„æŸ¥è¯¢:"
                for i, q in enumerate(questions, 1):
                    thinking_msg.content += f"\n{i}. {q.get('question', '')}"
                thinking_msg.metadata["status"] = "pending"

                log_msg.content += f"\n\n### ç­‰å¾…ç”¨æˆ·æ¾„æ¸…\n**é—®é¢˜æ•°**: {len(questions)}\n**é—®é¢˜**:\n"
                for i, q in enumerate(questions, 1):
                    log_msg.content += f"{i}. {q.get('question', '')}\n"

                yield [thinking_msg, log_msg]
                return  # ç­‰å¾…ç”¨æˆ·å›ç­”
            else:
                # è™½ç„¶éœ€è¦æ¾„æ¸…ï¼Œä½†æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆé—®é¢˜ï¼Œç»§ç»­ç ”ç©¶
                thinking_msg.content = "æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„æ¾„æ¸…é—®é¢˜ï¼Œç»§ç»­ç ”ç©¶..."
                log_msg.content += "\n\n### æŸ¥è¯¢åˆ†æ\n**ç»“æœ**: éœ€è¦æ¾„æ¸…ä½†æ— æœ‰æ•ˆé—®é¢˜\n"
                yield [thinking_msg, log_msg]
        else:
            # ä¸éœ€è¦æ¾„æ¸…ï¼Œç›´æ¥ç»§ç»­
            thinking_msg.content = "æŸ¥è¯¢å·²è¶³å¤Ÿæ¸…æ™°ï¼Œå¼€å§‹ç ”ç©¶..."
            log_msg.content += "\n\n### æŸ¥è¯¢åˆ†æ\n**ç»“æœ**: æŸ¥è¯¢æ¸…æ™°ï¼Œæ— éœ€æ¾„æ¸…\n"
            yield [thinking_msg, log_msg]

        # å±•ç¤ºç ”ç©¶é…ç½®
        thinking_msg.content = "æœç´¢ç›¸å…³ä¿¡æ¯ä¸­..."
        log_msg.content += f"\n\n### ç ”ç©¶é…ç½®\n**æœç´¢æä¾›å•†**: {search_source}\n"
        yield [thinking_msg, log_msg]

        # Track current plan and report for streaming
        report_active = False

        # Perform the research with streaming support
        async for partial_result in deep_research_stream(
                query=message,
                search_source=search_source,
                history_context=history_context
        ):
            # å¤„ç†ç ”ç©¶è¿›åº¦å’ŒçŠ¶æ€æ›´æ–°
            progress_update = await handle_research_progress(partial_result, thinking_msg, log_msg, conversation_state)
            if progress_update:
                yield progress_update

            # å¤„ç†æµå¼æŠ¥å‘Šå—
            if "final_report_chunk" in partial_result and not report_active:
                # ç¬¬ä¸€æ¬¡æ”¶åˆ°æŠ¥å‘Šå—ï¼Œåˆ›å»ºæŠ¥å‘Šæ¶ˆæ¯
                if not "report_msg" in locals():
                    report_msg = ChatMessage(
                        content="",
                        metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
                    )
                    yield [thinking_msg, log_msg, report_msg]

                # ç´¯ç§¯æŠ¥å‘Šå†…å®¹
                report_msg.content += partial_result["final_report_chunk"]
                yield [thinking_msg, log_msg, report_msg]

            # ä»ç ”ç©¶ç»“æœä¸­ç›´æ¥è·å–æœ€ç»ˆæŠ¥å‘Š
            elif "final_report" in partial_result and not report_active:
                report_active = True
                current_report = partial_result["final_report"]

                # å¦‚æœä¹‹å‰æ²¡æœ‰åˆ›å»ºæŠ¥å‘Šæ¶ˆæ¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª
                if not "report_msg" in locals():
                    report_msg = ChatMessage(
                        content=current_report,
                        metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
                    )
                else:
                    # ç¡®ä¿æŠ¥å‘Šå†…å®¹æ˜¯å®Œæ•´çš„
                    report_msg.content = current_report

                # Complete the thinking message
                thinking_msg.metadata["status"] = "done"

                log_msg.content += "\n\n### ç ”ç©¶å®Œæˆ\n**çŠ¶æ€**: ç”Ÿæˆäº†æœ€ç»ˆæŠ¥å‘Š\n**æ—¶é—´**: " + time.strftime(
                    '%H:%M:%S')
                yield [thinking_msg, log_msg, report_msg]

                # We now have everything, so we can break
                break

        # If we didn't get a final report but finished research, generate the report
        if not report_active:
            # Get the final results
            result = partial_result  # Use the last result from the stream
            learnings = result.get("learnings", [])

            # Show synthesis progress
            thinking_msg.content = "æ­£åœ¨æ•´åˆç ”ç©¶ç»“æœå’Œç”ŸæˆæŠ¥å‘Š..."

            log_msg.content += "\n\n### æ•´åˆç»“æœ\n**çŠ¶æ€**: åˆ†æå¹¶æ•´åˆæ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯\n**å‘ç°æ•°é‡**: " + str(
                len(learnings))
            yield [thinking_msg, log_msg]

            # Create report message
            report_msg = ChatMessage(
                content="æ­£åœ¨ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...",
                metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
            )
            yield [thinking_msg, log_msg, report_msg]

            # æ¸…é™¤æœ€åçŠ¶æ€ï¼Œé¿å…æŠ¥å‘Šç”ŸæˆçŠ¶æ€çš„é‡å¤
            conversation_state["last_status"] = ""

            # ç›´æ¥ç”ŸæˆæŠ¥å‘Šï¼Œä¸éœ€è¦é¢å¤–çš„ç­”æ¡ˆ
            report_content = ""
            async for report_chunk in write_final_report_stream(
                    query=message,
                    context=learnings,
                    history_context=history_context
            ):
                report_content += report_chunk
                report_msg.content = report_content
                yield [thinking_msg, log_msg, report_msg]

            # Complete the thinking message
            thinking_msg.metadata["status"] = "done"

            log_msg.content += "\n\n### ç ”ç©¶ç»“æŸ\n**çŠ¶æ€**: å…¨éƒ¨å®Œæˆ\n**æ—¶é—´**: " + time.strftime('%H:%M:%S')
            yield [thinking_msg, log_msg, report_msg]

    async def handle_clarification_answer(message, history_context):
        """Process the user's answers to clarification questions"""
        # Reset the waiting flag
        conversation_state["waiting_for_clarification"] = False

        # Get the original query and questions
        query = conversation_state["current_query"]
        questions = conversation_state["questions"]

        # é‡ç½®æœ€åçŠ¶æ€
        conversation_state["last_status"] = ""

        # Start with a thinking message - only show current step
        thinking_msg = ChatMessage(
            content="è§£ææ‚¨çš„æ¾„æ¸…å›ç­”...",
            metadata={"title": "_å¤„ç†ä¸­_", "id": 0, "status": "pending"}
        )

        # Always add a separate message for detailed logs
        log_msg = ChatMessage(
            content="## æ¾„æ¸…å¤„ç†è¯¦æƒ…\n\n_å¤„ç†ç”¨æˆ·å›ç­”çš„æ¾„æ¸…é—®é¢˜_",
            metadata={"title": "_ç ”ç©¶æ—¥å¿—_", "id": 100}
        )
        log_msg.content += f"\n\n### å¼€å§‹å¤„ç†\n**åŸå§‹æŸ¥è¯¢**: {query}\n**ç”¨æˆ·å›ç­”**: {message}\n**æ—¶é—´**: {time.strftime('%H:%M:%S')}\n"
        yield [thinking_msg, log_msg]

        # Simple parsing - assume one answer per line or comma-separated
        lines = [line.strip() for line in message.split('\n') if line.strip()]
        if len(lines) < len(questions):
            # Try comma separation if not enough lines
            if ',' in message:
                lines = [ans.strip() for ans in message.split(',')]

        # Create a dictionary of responses
        user_responses = {}
        for i, q in enumerate(questions):
            key = q.get("key", f"q{i}")
            if i < len(lines) and lines[i]:
                user_responses[key] = lines[i]

        # Process the clarifications
        thinking_msg.content = "å¤„ç†æ‚¨çš„æ¾„æ¸…..."

        log_msg.content += f"\n\n### è§£æå›ç­”\n**è§£æç»“æœ**: è·å–äº† {len(user_responses)}/{len(questions)} ä¸ªå›ç­”\n"
        yield [thinking_msg, log_msg]

        # Use await directly with the async function
        clarification_result = await process_clarifications(
            query=query,
            user_responses=user_responses,
            all_questions=questions,
            history_context=history_context
        )

        # Get refined query
        refined_query = clarification_result.get("refined_query", query)

        log_msg.content += f"\n\n### ä¼˜åŒ–æŸ¥è¯¢\n**åŸå§‹æŸ¥è¯¢**: {query}\n**ä¼˜åŒ–æŸ¥è¯¢**: {refined_query}\n"
        if clarification_result.get("assumptions"):
            log_msg.content += "**å‡è®¾**:\n- " + "\n- ".join(clarification_result.get("assumptions"))
        yield [thinking_msg, log_msg]

        # Check if direct answer is available
        if not clarification_result.get("requires_search", True) and clarification_result.get("direct_answer"):
            direct_answer = clarification_result.get("direct_answer", "")

            # Complete the thinking message
            thinking_msg.metadata["status"] = "done"

            # Create answer message
            report_msg = ChatMessage(
                content=direct_answer,
                metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
            )

            log_msg.content += f"\n\n### ç›´æ¥å›ç­”\n**çŠ¶æ€**: æŸ¥è¯¢å¯ä»¥ç›´æ¥å›ç­”ï¼Œæ— éœ€æœç´¢\n**æ—¶é—´**: {time.strftime('%H:%M:%S')}\n"
            yield [thinking_msg, log_msg, report_msg]
            return

        # Show research progress
        thinking_msg.content = "åŸºäºæ‚¨çš„æ¾„æ¸…æœç´¢ä¿¡æ¯..."

        log_msg.content += "\n\n### å¼€å§‹ç ”ç©¶\n**çŠ¶æ€**: éœ€è¦è¿›è¡Œæœç´¢\n"
        yield [thinking_msg, log_msg]

        # Track current report for streaming
        report_active = False
        partial_result = {}

        # Perform the research with streaming
        async for partial_result in deep_research_stream(
                query=refined_query,
                user_clarifications=user_responses,
                search_source=conversation_state.get("search_source", "serper"),
                history_context=history_context
        ):
            # å¤„ç†ç ”ç©¶è¿›åº¦å’ŒçŠ¶æ€æ›´æ–°
            progress_update = await handle_research_progress(partial_result, thinking_msg, log_msg, conversation_state)
            if progress_update:
                yield progress_update

            # å¤„ç†æµå¼æŠ¥å‘Šå—
            if "final_report_chunk" in partial_result and not report_active:
                # ç¬¬ä¸€æ¬¡æ”¶åˆ°æŠ¥å‘Šå—ï¼Œåˆ›å»ºæŠ¥å‘Šæ¶ˆæ¯
                if not "report_msg" in locals():
                    report_msg = ChatMessage(
                        content="",
                        metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
                    )
                    yield [thinking_msg, log_msg, report_msg]

                # ç´¯ç§¯æŠ¥å‘Šå†…å®¹
                report_msg.content += partial_result["final_report_chunk"]
                yield [thinking_msg, log_msg, report_msg]

            # ä»ç ”ç©¶ç»“æœä¸­ç›´æ¥è·å–æœ€ç»ˆæŠ¥å‘Š
            elif "final_report" in partial_result and not report_active:
                report_active = True
                current_report = partial_result["final_report"]

                # å¦‚æœä¹‹å‰æ²¡æœ‰åˆ›å»ºæŠ¥å‘Šæ¶ˆæ¯ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª
                if not "report_msg" in locals():
                    report_msg = ChatMessage(
                        content=current_report,
                        metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
                    )
                else:
                    # ç¡®ä¿æŠ¥å‘Šå†…å®¹æ˜¯å®Œæ•´çš„
                    report_msg.content = current_report

                # Complete the thinking message
                thinking_msg.metadata["status"] = "done"

                log_msg.content += "\n\n### ç ”ç©¶å®Œæˆ\n**çŠ¶æ€**: ç”Ÿæˆäº†æœ€ç»ˆæŠ¥å‘Š\n**æ—¶é—´**: " + time.strftime(
                    '%H:%M:%S')
                yield [thinking_msg, log_msg, report_msg]

                # We now have everything, so we can break
                break

        # If we didn't get a final report but finished research, generate the report
        if not report_active:
            # Get the final results
            result = partial_result  # Use the last result from the stream
            learnings = result.get("learnings", [])

            # Show synthesis progress
            thinking_msg.content = "æ­£åœ¨æ•´åˆç ”ç©¶ç»“æœå’Œç”ŸæˆæŠ¥å‘Š..."

            log_msg.content += "\n\n### æ•´åˆç»“æœ\n**çŠ¶æ€**: åˆ†æå¹¶æ•´åˆæ‰€æœ‰æ”¶é›†çš„ä¿¡æ¯\n**å‘ç°æ•°é‡**: " + str(
                len(learnings))
            yield [thinking_msg, log_msg]

            # Create report message
            report_msg = ChatMessage(
                content="æ­£åœ¨ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...",
                metadata={"title": "_ç ”ç©¶æŠ¥å‘Š_", "id": 2}
            )
            yield [thinking_msg, log_msg, report_msg]

            # æ¸…é™¤æœ€åçŠ¶æ€ï¼Œé¿å…æŠ¥å‘Šç”ŸæˆçŠ¶æ€çš„é‡å¤
            conversation_state["last_status"] = ""

            # ç›´æ¥ç”ŸæˆæŠ¥å‘Šï¼Œä¸éœ€è¦é¢å¤–çš„ç­”æ¡ˆ
            report_content = ""
            async for report_chunk in write_final_report_stream(
                    query=refined_query,
                    context=learnings,
                    history_context=history_context
            ):
                report_content += report_chunk
                report_msg.content = report_content
                yield [thinking_msg, log_msg, report_msg]

            # Complete the thinking message
            thinking_msg.metadata["status"] = "done"

            log_msg.content += "\n\n### ç ”ç©¶ç»“æŸ\n**çŠ¶æ€**: å…¨éƒ¨å®Œæˆ\n**æ—¶é—´**: " + time.strftime('%H:%M:%S')
            yield [thinking_msg, log_msg, report_msg]

    # Create a modern interface using ChatInterface
    demo = gr.ChatInterface(
        fn=research_with_thinking,
        title="ğŸ” Deep Research",
        description="""ä½¿ç”¨æ­¤å·¥å…·è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Œæˆ‘å°†æœç´¢äº’è”ç½‘ä¸ºæ‚¨æ‰¾åˆ°å›ç­”ã€‚Powered by <a href="https://github.com/shibing624/deep-research" target="_blank">Deep Research</a> Made with â¤ï¸ by <a href="https://github.com/shibing624" target="_blank">shibing624</a>""",
        additional_inputs=[
            gr.Dropdown(
                choices=["tavily", "serper", "mp_search"],
                value="tavily",
                label="æœç´¢æä¾›å•†",
                info="è¦ä½¿ç”¨çš„æœç´¢å¼•æ“"
            )
        ],
        examples=[
            ["ç‰¹æ–¯æ‹‰è‚¡ç¥¨çš„æœ€æ–°è¡Œæƒ…?"],
            ["How does climate change affect biodiversity?"],
            ["ä¸­å›½2024å¹´GDPå¢é•¿äº†å¤šå°‘?"],
            ["Explain the differences between supervised and unsupervised machine learning."]
        ],
        type="messages"
    )

    # Launch the demo
    demo.queue()
    demo.launch(server_name="0.0.0.0", share=False)


if __name__ == "__main__":
    run_gradio_demo()
